{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228d1cd",
   "metadata": {},
   "source": [
    "### Reading the 100x100trail Data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_100x100_df = pd.read_excel(f'Database Data/TORX_df_100x100_trail.xlsx' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef326e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_itra_no_DNF = pd.read_excel(f'Database Data/TORX_itra_no_DNF.xlsx',\n",
    "                                dtype={'Start Date': 'string'})\n",
    "\n",
    "# # Converting the date columns to datetime format with the specified format\n",
    "# TORX_itra_no_DNF['Start Date'] = pd.to_datetime(\n",
    "#     TORX_itra_no_DNF['Start Date'], \n",
    "#     format='%Y-%m-%d %H:%M:%S'\n",
    "# )\n",
    "\n",
    "# Convert 'Time' to timedelta format\n",
    "TORX_itra_no_DNF['Performance'] = pd.to_timedelta(TORX_itra_no_DNF['Performance'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "# TORX_duv_df['DUV_Performance'] = TORX_duv_df['Finish Time for Tableau'] - TORX_duv_df['Start Date']\n",
    "TORX_itra_no_DNF = TORX_itra_no_DNF.rename(columns={\\\"Performance\\\": \\\"ITRA_Performance\\\",\n",
    "                                          \\\"Performance_Seconds\\\": \\\"ITRA_Performance_Seconds\\\",\n",
    "                                         }) \n",
    "\n",
    "TORX_itra_no_DNF.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e299e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_duv_df = pd.read_excel(f'Database Data/TORX_duv_df.xlsx')\\n\",\n",
    "\n",
    "# Convert 'Time' to timedelta format\\n\",\n",
    "TORX_duv_df['Performance'] = pd.to_timedelta(TORX_duv_df['Performance'], errors='coerce')\\n\",\n",
    "\n",
    "# Converting the date columns to datetime format with the specified format\\n\",\n",
    "TORX_duv_df['Start Date'] = pd.to_datetime(\n",
    "    TORX_duv_df['Start Date'],\n",
    "    format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# TORX_duv_df['DUV_Performance'] = TORX_duv_df['Finish Time for Tableau'] - TORX_duv_df['Start Date']\\n\",\n",
    "TORX_duv_df = TORX_duv_df.rename(columns={\\\"Performance\\\": \\\"DUV_Performance\\\"\n",
    "                \\\"Performance\\\": \\\"DUV_Performance_Seconds\\\"\n",
    "                    }) \\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f939a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e952fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = ['2021', '2022', '2023', '2023']\n",
    "races = ['TOR450', 'TOR330' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0704d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for race in races:\n",
    "    for year in years:\n",
    "        print(race, year, '\\\\n')\n",
    "        itra_df = TORX_itra_no_DNF[(TORX_itra_no_DNF['Race'] == race) &\n",
    "                                   (TORX_itra_no_DNF['Year'] == year) &\n",
    "                                   # not looking at people who Finished at Rifugio Frassati or Bossess\n",
    "                                   (TORX_itra_no_DNF['Status'] == 'Finished') \n",
    "                                  ]\n",
    "        itra_num_rows = itra_df.shape[0]\n",
    "        print(\\\"ITRA rows:\\\", itra_num_rows)\n",
    "    \n",
    "        ##################################\n",
    "        duv_df = TORX_duv_df[(TORX_duv_df['Race'] == race) &\n",
    "                           (TORX_duv_df['Year'] == year) ]\n",
    "        duv_num_rows = duv_df.shape[0]\n",
    "        print(\\\"DUV rows:\\\", duv_num_rows)\n",
    "        \n",
    "        ##################################\n",
    "        trail_df = TORX_100x100_df[(TORX_100x100_df['Race'] == race) &\n",
    "                           (TORX_100x100_df['Year'] == year) &\n",
    "                                  (TORX_100x100_df['Status'] == True)]       \n",
    "        trail_num_rows = trail_df.shape[0]\n",
    "        print(\\\"100x100 rows:\\\", trail_num_rows)\n",
    "        ##################################\n",
    "\n",
    "        print(itra_num_rows == duv_num_rows)\n",
    "        print(itra_num_rows == trail_num_rows)\n",
    "        print('*'*20)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [TORX_100x100_df,\n",
    "            TORX_duv_df,\n",
    "            TORX_itra_no_DNF]\n",
    "\n",
    "for df in datasets: \n",
    "    df['Year'] = df['Year'].astype('str')\n",
    "    df['Name'] = df['Name'].str.replace('-', ' ')\n",
    "    df['Name'] = df['Name'].str.replace(r\\\"\\\\s+\\\", \\\" \\\", regex=True)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b34b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Examining which dataset does not have the same number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9cc9a",
   "metadata": {},
   "source": [
    "#### Finding similar names for all finishers in 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa25cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "itra_df = TORX_itra_no_DNF[(TORX_itra_no_DNF['Race'] == race) &\n",
    "                           (TORX_itra_no_DNF['Year'] == year) &\n",
    "                           # not looking at people who Finished at Rifugio Frassati or Bossess\n",
    "                           (TORX_itra_no_DNF['Status'] == 'Finished') \n",
    "                          ]\n",
    "\n",
    "duv_df = TORX_duv_df[(TORX_duv_df['Race'] == race) &\n",
    "                   (TORX_duv_df['Year'] == year) ]\n",
    "\n",
    "\n",
    "trail_df = TORX_100x100_df[(TORX_100x100_df['Race'] == race) &\n",
    "                   (TORX_100x100_df['Year'] == year) &\n",
    "                          (TORX_100x100_df['Status'] == True)\n",
    "                         ]   \n",
    "   \n",
    "print(itra_df.shape)\n",
    "print(duv_df.shape)\n",
    "print(trail_df.shape)\"\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to track issues with names\n",
    "name_issues = {}\n",
    "name_issues_dataset = {}\n",
    "\n",
    "dataset_names = ['TORX_100x100', 'TORX_duv', 'TORX_itra_no_DNF']  # Corresponding names of the datasets\n",
    "\n",
    "# Create a dictionary to store the dataset origin for each name\n",
    "name_issues_with_dataset = {}\n",
    "\n",
    "for dataset, dataset_name in zip(datasets, dataset_names):\n",
    "    unique_name = dataset['Name'].unique()\n",
    "    print(f\\\"Processing {dataset_name}, Unique Names: {len(unique_name)}\\\")\n",
    "\n",
    "    for name in unique_name: \n",
    "        name_split = name.split(' ')\n",
    "        name1 = name_split[0]\n",
    "        name2 = name_split[1]\n",
    "        name_issues_with_dataset[name] = {'variations': [], 'dataset': dataset_name}\n",
    "\n",
    "        try:\n",
    "            if len(name_split) == 2:\n",
    "                new_name = name2 + ' ' + name1\n",
    "                name_issues_with_dataset[name]['variations'].append(name)\n",
    "                name_issues_with_dataset[name]['variations'].append(new_name)\n",
    "\n",
    "            elif len(name_split) == 3:\n",
    "                # Regex pattern extracting 'D Haene Francois'  \n",
    "                pattern = r\\\"\\\\b[A-Za-z]\\\\s[A-Za-z]+\\\\s[A-Za-z]+\\\\b\\\"\n",
    "\n",
    "                name3 = name_split[2]\n",
    "                if name3 == name1:\n",
    "                    new_name = name1 + ' ' + name2\n",
    "                    name_issues_with_dataset[name]['variations'].append(new_name)\n",
    "\n",
    "                # D Angelo Yuri ['DAngelo Yuri','Yuri DAngelo']\n",
    "                elif pd.Series([name]).str.contains(pattern, regex=True).any():\n",
    "                    new_name = name.replace(f\\\"{name1} \\\", f\\\"{name1}\\\")\n",
    "                    name_issues_with_dataset[name]['variations'].append(new_name)\n",
    "    \n",
    "                    name_split = new_name.split(' ')\n",
    "                    name1 = name_split[0]\n",
    "                    name2 = name_split[1]\n",
    "                    new_name = name2 + ' ' + name1\n",
    "                    name_issues_with_dataset[name]['variations'].append(new_name)\n",
    "\n",
    "                else:\n",
    "                    new_name1 = name1 + ' ' + name2\n",
    "                    new_name2 = name1 + ' ' + name3\n",
    "                    name_issues_with_dataset[name]['variations'].append(new_name1)\n",
    "                    name_issues_with_dataset[name]['variations'].append(new_name2)\n",
    "\n",
    "            elif len(name_split) > 3:\n",
    "                name_issues_with_dataset[name]['variations'].append(name)\n",
    "\n",
    "            else:\n",
    "                name_issues_with_dataset[name]['variations'].append(name)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\\\"Issue: {name}\\\\nError: {e}\\\\n\\\")\n",
    "            name_issues_with_dataset[name]['variations'].append(name)\n",
    "\n",
    "# Convert name issues with dataset to DataFrame for easier tracking\n",
    "name_issues_df = pd.DataFrame([{\n",
    "    'Name': k,\n",
    "    'Variations': v['variations'],\n",
    "    'Dataset': v['dataset']\n",
    "} for k, v in name_issues_with_dataset.items()])\n",
    "\n",
    "print(\\\"Name Issues DataFrame with Dataset Info:\\\")\n",
    "print(name_issues_df)\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e035e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each name and variations\n",
    "for idx, name in enumerate(name_issues_df['Name']):\n",
    "    # Access the current list of variations\n",
    "    current_variations = name_issues_df.at[idx, 'Variations']\n",
    "    # reorder in a alphabetical order\n",
    "    current_variations = sorted(current_variations)\n",
    "    print(current_variations)\n",
    "    \n",
    "    name_issues_df.at[idx, 'Variations'] = current_variations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a function to normalize names\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ad63ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(name,  name_issues_df ):\n",
    "    # Find the variation list corresponding to the name\n",
    "    match =  name_issues_df [ name_issues_df ['Name'] == name]\n",
    "    if not match.empty:\n",
    "        return match['Variations'].values[0]\n",
    "    return [name]  # If no variation is found, return the name as is.\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Step 2: Apply the name normalization to both datasets\n",
    "    # Normalize dataset1\n",
    "    dataset['Name Variations'] = dataset['Name'].apply(lambda x: normalize_name(x, name_issues_df))\n",
    "\n",
    "TORX_100x100_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f72e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Iterate through the names in Dataset1 and Dataset2 to find matches\n",
    "merged_rows = []\n",
    "\n",
    "def testing_df(name):\n",
    "    itra_name_df = TORX_itra_no_DNF[TORX_itra_no_DNF['Name'].str.contains(name)]\n",
    "#     duv_name_df = TORX_duv_df[TORX_duv_df['Name'].str.contains(name)]\n",
    "    trail_name_df = TORX_100x100_df[TORX_100x100_df['Name'].str.contains(name)]\n",
    "\n",
    "    for idx1, row_itra in itra_name_df.iterrows():\n",
    "\n",
    "        for idx2, row_trail in trail_name_df .iterrows():\n",
    "            # Check if the name from itra_verjee_df is in the variations of trail_verjee_df and vice versa\n",
    "            if any(name in row_trail['Name Variations'] for name in row_itra['Name Variations']):\n",
    "\n",
    "                if  row_trail['Year'] ==  row_itra['Year']  :\n",
    "#                     print('Same year:', row_trail['Year'], row_itra['Year'], '\\\\n', '*'*20)\n",
    "    #                 print('row_itra',row_itra)\n",
    "    #                 print('row_trail',row_trail)\n",
    "\n",
    "                    # Merge the row if a match is found\n",
    "                    merged_row = {**row_itra.to_dict(), **row_trail.to_dict()}\n",
    "                    merged_rows.append(merged_row)\n",
    "    \n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "#     print(merged_rows)\n",
    "    # Step 4: Create a merged DataFrame\n",
    "    merged_df = pd.DataFrame(merged_rows)\n",
    "\n",
    "    merged_df = merged_df.drop_duplicates(subset=['Name','Year','Start Date'])\n",
    "\n",
    "\n",
    "    # Display the merged DataFrame\n",
    "    print(merged_df)\n",
    "    \n",
    "testing_df('Verjee')\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d58c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_itra_no_DNF = pd.read_excel(f'Database Data/TORX_itra_no_DNF.xlsx',\n",
    "                                dtype={'Start Date': 'string'})\n",
    "\n",
    "# Converting the date columns to datetime format with the specified format\n",
    "TORX_itra_no_DNF['Start Date'] = pd.to_datetime(\n",
    "    TORX_itra_no_DNF['Start Date'], \n",
    "    format='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Convert 'Time' to timedelta format\n",
    "TORX_itra_no_DNF['Performance'] = pd.to_timedelta(TORX_itra_no_DNF['Performance'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "# TORX_duv_df['DUV_Performance'] = TORX_duv_df['Finish Time for Tableau'] - TORX_duv_df['Start Date']\n",
    "TORX_itra_no_DNF = TORX_itra_no_DNF.rename(columns={\\\"Performance\\\": \\\"ITRA_Performance\\\",\n",
    "                                          \\\"Performance_Seconds\\\": \\\"ITRA_Performance_Seconds\\\",\n",
    "                                         }) \n",
    "\n",
    "TORX_itra_no_DNF.head(50)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(years)\n",
    "sub_TORX_itra_no_DNF = TORX_itra_no_DNF[(TORX_itra_no_DNF['Year'].isin(years)) &\n",
    "                                        (TORX_itra_no_DNF['Status']== 'Finished')\n",
    "                                       ]\n",
    "sub_TORX_100x100_df = TORX_100x100_df[(TORX_100x100_df['Year'].isin(years)) &\n",
    "                                      (TORX_100x100_df['Race'] !=  'TOR130') &\n",
    "                                      (TORX_100x100_df['Status'] ==  True) \n",
    "                                     ]\n",
    "\n",
    "print(sub_TORX_itra_no_DNF.shape)\n",
    "print(sub_TORX_100x100_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df, subname_list = testing_df(sub_TORX_itra_no_DNF, sub_TORX_100x100_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df_names = merged_df['Name'].unique() \n",
    "# sub_names =sub_TORX_100x100_df['Name'].unique()\n",
    "# print(len(merged_df_names))\n",
    "# print(len(sub_names))\n",
    "\n",
    "# subname_list = []\n",
    "# for sub_name in sub_names:\n",
    "#     if sub_name in  merged_df_names:\n",
    "#         pass\n",
    "#     else:\n",
    "#         subname_list.append(sub_name)\n",
    "        \n",
    "# print(len(subname_list))\n",
    "        \n",
    "        \n",
    "# sub_TORX_100x100_df[sub_TORX_100x100_df['Name'].isin(subname_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cab5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(merged_df['Name'][\n",
    "    (merged_df['ITRA_Performance_Seconds'].isna()) |\n",
    "    (merged_df['DUV_Performance_Seconds'].isna())\n",
    "    ].unique()))\"\n",
    "\n",
    "TORX_itra_no_DNF[TORX_itra_no_DNF['Name'] == 'Corsini Simone']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d2e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a95d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25529348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f631a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6abe8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850993b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07612ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4d1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce6302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1148ac6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04257ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941ffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a0b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb03a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddafd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf2a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f55e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
