{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b17f9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f27f311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOR_330_dem = pd.read_excel(f'TOR330 Data/5. Clean Data for Data Visualisation/TOR330_dem.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "625a883a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year  Status1                     \n",
       "2021  DNF                             286\n",
       "      Finished                        431\n",
       "2022  DNF                             363\n",
       "      Finished at Bosses               88\n",
       "      Finished at Courmayeur          408\n",
       "      Finished at Rifugio Frassati    101\n",
       "2023  DNF                             585\n",
       "      Finished                        622\n",
       "2024  DNF                             563\n",
       "      Finished                        533\n",
       "Name: Status1, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOR_330_dem.groupby( ['Year','Status1'] )['Status1' ].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d69cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbf473db",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'TOR330 Data/5. Clean Data for Data Visualisation/TOR330_checkpoints_bibs.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m checkpoints_bib_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTOR330 Data/5. Clean Data for Data Visualisation/TOR330_checkpoints_bibs.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:457\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    456\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1376\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1376\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m         )\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1250\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1248\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1253\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1254\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\common.py:795\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    787\u001b[0m             handle,\n\u001b[0;32m    788\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    791\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    798\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'TOR330 Data/5. Clean Data for Data Visualisation/TOR330_checkpoints_bibs.xlsx'"
     ]
    }
   ],
   "source": [
    "checkpoints_bib_df = pd.read_excel(f'TOR330 Data/5. Clean Data for Data Visualisation/TOR330_checkpoints_bibs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aid_station_bib_df = pd.read_excel(f'TOR330 Data/5. Clean Data for Data Visualisation/TOR330_all_aid_station_bib_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e327c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifebase_bib_df = pd.read_excel(f'TOR330 Data/5. Clean Data for Data Visualisation/TOR330_lifebase_bib.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making duration hours \n",
    "datasets = [checkpoints_bib_df, lifebase_bib_df, all_aid_station_bib_df,  TOR_330_dem]\n",
    "for df in datasets:\n",
    "    df['Duration_hours'] = df['Duration_seconds']/ 3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c20c80",
   "metadata": {},
   "source": [
    "### Getting Average and Median time for lifebases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_time_stats(df, column, category_order):\n",
    "    df_merge = df.merge(\n",
    "        TOR_330_dem[['PK', 'Finish Category']],\n",
    "        on=['PK'],\n",
    "        how='left')\n",
    "\n",
    "\n",
    "    df_merge['Duration_seconds'][df_merge['Duration_seconds']  <= 0] = np.nan\n",
    "\n",
    "    stats_df = df_merge.groupby(['Finish Category', column])['Duration_seconds'].describe().reset_index(drop = False)\n",
    "\n",
    "\n",
    "    stats_df[[ 'mean', 'std', 'min', '25%',\n",
    "           '50%', '75%', 'max']] = stats_df[[ 'mean', 'std', 'min', '25%',\n",
    "           '50%', '75%', 'max']].round(0)\n",
    "\n",
    "    # Set 'Finish Category' as a categorical column with the defined order\n",
    "    stats_df[column] = pd.Categorical(\n",
    "        stats_df[column],\n",
    "        categories=category_order,\n",
    "        ordered=True)\n",
    "    \n",
    "    stats_df = stats_df.sort_values(by=column, ascending = True)\n",
    "    \n",
    "    \n",
    "\n",
    "    for finish_category in df_merge['Finish Category'].unique():\n",
    "        print(finish_category)\n",
    "        stats_df.loc[\n",
    "                (stats_df['Finish Category'] == finish_category), 'running_total_mean_seconds'\n",
    "            ] =     stats_df.loc[\n",
    "                (stats_df['Finish Category'] == finish_category), 'mean'\n",
    "            ].cumsum()\n",
    "\n",
    "\n",
    "        stats_df.loc[\n",
    "                (stats_df['Finish Category'] == finish_category), 'running_total_median_seconds'\n",
    "            ] =     stats_df.loc[\n",
    "                (stats_df['Finish Category'] == finish_category), '50%'\n",
    "            ].cumsum()\n",
    "\n",
    "\n",
    "    stats_df = stats_df[['Finish Category', column,\n",
    "            'count', 'mean', '50%', 'std', 'min', 'max', \n",
    "            'running_total_mean_seconds', 'running_total_median_seconds']]\n",
    "\n",
    "\n",
    "    stats_df = stats_df.rename(columns={'count': f'Count_Finish_Category_{column}_seconds',\n",
    "                                      'mean': f'Mean_Finish_Category_{column}_seconds',\n",
    "                                      'std': f'STD_Finish_Category{column}t_seconds',\n",
    "                                      '50%': f'Median_Finish_Category_{column}_seconds',\n",
    "                                      'min': f'Min_Finish_Category_{column}_seconds', \n",
    "                                      'max': f'Max_Finish_Category_{column}_seconds'})\n",
    "    \n",
    "    stats_df.to_excel(f'{race} Data/5. Clean Data for Data Visualisation/{race}_{column}_duration_stats_df.xlsx', index = False)\n",
    "\n",
    "#     print(stats_df[stats_df['Finish Category'] == 'Sub-130'])\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifebase_stats_df = creating_time_stats(df = lifebase_bib_df, \n",
    "                    column = 'Lifebase', \n",
    "                    category_order = ['Start',\n",
    "                            'Valgrisenche IN','Valgrisenche OUT',\n",
    "                            'Cogne IN',  'Cogne OUT',\n",
    "                            'Donnas IN', 'Donnas OUT', \n",
    "                            'Gressoney IN','Gressoney OUT',\n",
    "                            'Valtournenche IN','Valtournenche OUT', \n",
    "                            'Ollomont IN','Ollomont OUT',\n",
    "                            'FINISH'])\n",
    "\n",
    "lifebase_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac72d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_stats_df = creating_time_stats(df = checkpoints_bib_df, \n",
    "                    column = 'Checkpoint', \n",
    "                    category_order = ['Start', 'La Thuile', 'Valgrisenche IN', 'Valgrisenche OUT',\n",
    "                                       'Eaux Rousse', 'Cogne IN', 'Cogne OUT', 'Donnas IN', 'Donnas OUT',\n",
    "                                       'Rifugio della Barma', 'Niel La Gruba', 'Gressoney IN',\n",
    "                                       'Gressoney OUT', 'Champoluc', 'Valtournenche IN',\n",
    "                                       'Valtournenche OUT', 'Oyace', 'Ollomont IN', 'Ollomont OUT',\n",
    "                                       'FINISH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aid_station_bib_stats_df = creating_time_stats(df = all_aid_station_bib_df, \n",
    "                    column = 'Aid Station', \n",
    "                    category_order = ['Start',\n",
    "                                    'Baite Youlaz', 'La Thuile', 'Rifugio Deffeyes',\n",
    "                                   'Planaval', 'Valgrisenche IN', 'Valgrisenche OUT', 'Chalet Epee',\n",
    "                                   'Rhemes-Notre-Dame', 'Eaux Rousse', 'Rifugio Sella', 'Cogne IN',\n",
    "                                   'Cogne OUT', 'Goilles', 'Rifugio Dondena', 'Chardonney', 'Pontboset',\n",
    "                                   'Donnas IN', 'Donnas OUT', 'Perloz', 'Sassa', 'Rifugio Coda',\n",
    "                                   'Rifugio della Barma', 'Lago Chiaro', 'Col della Vecchia',\n",
    "                                   'Niel La Gruba', 'Loo', 'Gressoney IN', 'Gressoney OUT',\n",
    "                                   'Rifugio Alpenzu', 'Champoluc', 'Rifugio Grand Tournalin',\n",
    "                                   'Valtournenche IN', 'Valtournenche OUT', 'Rifugio Barmasse', 'Vareton',\n",
    "                                   'Rifugio Magià', 'Rifugio Cuney', 'Bivacco R. Clermont', 'Oyace',\n",
    "                                   'Bruson Arp', 'Ollomont IN', 'Ollomont OUT', 'Rifugio Champillon',\n",
    "                                   'Ponteille Desot', 'Bosses', 'Rifugio Frassati', 'Pas Entre Deux Sauts',\n",
    "                                   'Monte de la Saxe', 'FINISH'])\n",
    "\n",
    "all_aid_station_bib_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72508766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475bf990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d36875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb8efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cde07a13",
   "metadata": {},
   "source": [
    "### Who ran too easy or hard at the start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_TOR_330_dem_bib_list = list(TOR_330_dem['PK'][TOR_330_dem['Finish Category'] == 'Sub-130'].unique())\n",
    "\n",
    "# new_bib_list = []\n",
    "# for bib in sub_TOR_330_dem_bib_list:\n",
    "#     df = lifebase_bib_df[\n",
    "#                 ((lifebase_bib_df['Duration_hours']> 14) &\n",
    "#                 (lifebase_bib_df['Lifebase'] == 'Valgrisenche IN') )\n",
    "#     &\n",
    "#                 (lifebase_bib_df['PK']== bib)\n",
    "#                ]\n",
    "    \n",
    "#     new_bib_list.append(df)\n",
    "# df= pd.concat(new_bib_list)\n",
    "# pk_unique = list(df['PK'].unique())\n",
    "\n",
    "# for pk in pk_unique:\n",
    "#     print(lifebase_bib_df[['PK','Lifebase', 'Timestamp', 'Duration_hours']][(lifebase_bib_df['PK'] == pk)  ], '\\n', '*'*40,  '\\n',)\n",
    "#     print(TOR_330_dem[['PK', 'Finish Category', 'Duration_hours']][(TOR_330_dem['PK'] == pk)  ], '\\n','\\n','\\n','\\n', '*'*40,  '\\n',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaee483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_TOR_330_dem_bib_list = list(TOR_330_dem['PK'][TOR_330_dem['Finish Category'] == 'Sub-140'].unique())\n",
    "\n",
    "# new_bib_list = []\n",
    "# for bib in sub_TOR_330_dem_bib_list:\n",
    "#     df = lifebase_bib_df[\n",
    "#                 ((lifebase_bib_df['Duration_hours']< 10) &\n",
    "#                 (lifebase_bib_df['Lifebase'] == 'Valgrisenche IN') ) &\n",
    "#                 (lifebase_bib_df['PK']== bib)]\n",
    "#     new_bib_list.append(df)\n",
    "    \n",
    "# df= pd.concat(new_bib_list)\n",
    "\n",
    "# pk_unique = list(df['PK'].unique())\n",
    "\n",
    "# for pk in pk_unique:\n",
    "#     print(lifebase_bib_df[['PK', 'Wave', 'Lifebase', 'Duration_hours']][(lifebase_bib_df['PK'] == pk)  ], '\\n', '\\n',)\n",
    "#     print(TOR_330_dem[['PK', 'Finish Category', 'Duration_hours']][(TOR_330_dem['PK'] == pk)  ], '\\n', '*'*40,  '\\n',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reading in Raw Data\n",
    "# races = ['TOR330']\n",
    "# years = [ \n",
    "# #     '2021',\n",
    "# #         '2022',\n",
    "# #          '2023', \n",
    "#     '2024'\n",
    "#         ]\n",
    "\n",
    "# TORX_df = {}\n",
    "\n",
    "# for race in races:\n",
    "#     for year in years:\n",
    "#         df = pd.read_excel(f'{race} Data/1. 100x100trail/{race}_{year}.xlsx',\n",
    "#                                  dtype={'Start Date': 'string',\n",
    "#                                         'Year': 'string'})\n",
    "#         print(f'{race}_{year} {df.shape}')\n",
    "#         # Store the DataFrame in the dictionary with a key like 'TOR330_2021'\n",
    "#         TORX_df[f'{race}_{year}'] = df\n",
    "#     print('*'*50)\n",
    "    \n",
    "# TORX_df_concat = pd.concat(TORX_df)\n",
    "# TOR330 = TORX_df_concat[TORX_df_concat['Year'] == year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a170581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_TOR_330_dem_bib_list = list(TOR_330_dem['PK'][TOR_330_dem['Finish Category'] == 'Sub-90'].unique())\n",
    "\n",
    "# new_bib_list = []\n",
    "# for bib in sub_TOR_330_dem_bib_list:\n",
    "#     df = lifebase_bib_df[\n",
    "#                 ((lifebase_bib_df['Duration_hours']> 8) &\n",
    "#                 (lifebase_bib_df['Lifebase'] == 'Valgrisenche OUT') ) &\n",
    "#                 (lifebase_bib_df['PK']== bib)]\n",
    "#     new_bib_list.append(df)\n",
    "    \n",
    "# df= pd.concat(new_bib_list)\n",
    "\n",
    "# pk_unique = list(df['PK'].unique())\n",
    "\n",
    "# for pk in pk_unique:\n",
    "#     print(lifebase_bib_df[['PK', 'Wave', 'Lifebase', 'Duration_hours']][(lifebase_bib_df['PK'] == pk)  ], '\\n', '\\n',)\n",
    "#     print(TOR_330_dem[['PK', 'Finish Category', 'Duration_hours']][(TOR_330_dem['PK'] == pk)  ], '\\n', '*'*40,  '\\n',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9dd748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8200ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3812c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf6533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
