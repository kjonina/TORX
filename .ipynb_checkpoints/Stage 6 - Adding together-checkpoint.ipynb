{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47edb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c2ba7",
   "metadata": {},
   "source": [
    "### Reading the 100x100trail Data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_100x100_df = pd.read_excel(f'Database Data/TORX_df_100x100_trail.xlsx' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34829f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_100x100_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b6d27",
   "metadata": {},
   "source": [
    "### DUV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the result\n",
    "TORX_duv_df = pd.read_excel(f'Database Data/TORX_duv_df.xlsx')\n",
    "\n",
    "# Convert 'Time' to timedelta format\n",
    "TORX_duv_df['Performance'] = pd.to_timedelta(TORX_duv_df['Performance'], errors='coerce')\n",
    "\n",
    "\n",
    "# Converting the date columns to datetime format with the specified format\n",
    "TORX_duv_df['Start Date'] = pd.to_datetime(\n",
    "    TORX_duv_df['Start Date'], \n",
    "    format='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# TORX_duv_df['DUV_Performance'] = TORX_duv_df['Finish Time for Tableau'] - TORX_duv_df['Start Date']\n",
    "TORX_duv_df = TORX_duv_df.rename(columns={\"Performance\": \"DUV_Performance\",\n",
    "                                          \"Performance\": \"DUV_Performance_Seconds\",\n",
    "                                         }) \n",
    "\n",
    "TORX_duv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_duv_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa7b35",
   "metadata": {},
   "source": [
    "### ITRA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd48731",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_itra_no_DNF = pd.read_excel(f'Database Data/TORX_itra_no_DNF.xlsx',\n",
    "                                dtype={'Start Date': 'string'})\n",
    "\n",
    "# Converting the date columns to datetime format with the specified format\n",
    "TORX_itra_no_DNF['Start Date'] = pd.to_datetime(\n",
    "    TORX_itra_no_DNF['Start Date'], \n",
    "    format='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Convert 'Time' to timedelta format\n",
    "TORX_itra_no_DNF['Performance'] = pd.to_timedelta(TORX_itra_no_DNF['Performance'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "# TORX_duv_df['DUV_Performance'] = TORX_duv_df['Finish Time for Tableau'] - TORX_duv_df['Start Date']\n",
    "TORX_itra_no_DNF = TORX_itra_no_DNF.rename(columns={\"Performance\": \"ITRA_Performance\",\n",
    "                                          \"Performance_Seconds\": \"ITRA_Performance_Seconds\",\n",
    "                                         }) \n",
    "\n",
    "TORX_itra_no_DNF.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c27380",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_itra_no_DNF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71287cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_itra_no_DNF['Status'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557bb66c",
   "metadata": {},
   "source": [
    "## Defining Varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7425bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [TORX_100x100_df,\n",
    "            TORX_duv_df,\n",
    "            TORX_itra_no_DNF\n",
    "           ]\n",
    "\n",
    "year = ['2021', '2022', '2023', '2023']\n",
    "races = ['TOR450', 'TOR330' ]\n",
    "years = ['2021', '2022', '2023', '2024']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f173a3a3",
   "metadata": {},
   "source": [
    "### Cleaning Year and Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in datasets: \n",
    "    df['Year'] = df['Year'].astype('str')\n",
    "    df['Name'] = df['Name'].str.replace('-', ' ')\n",
    "    df['Name'] = df['Name'].str.replace(r\"\\s+\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72137ab7",
   "metadata": {},
   "source": [
    "### Examining which dataset does not have the same number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b46ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for race in races:\n",
    "    for year in years:\n",
    "        print(race, year, '\\n')\n",
    "        itra_df = TORX_itra_no_DNF[(TORX_itra_no_DNF['Race'] == race) &\n",
    "                                   (TORX_itra_no_DNF['Year'] == year) &\n",
    "                                   # not looking at people who Finished at Rifugio Frassati or Bossess\n",
    "                                   (TORX_itra_no_DNF['Status'] == 'Finished') \n",
    "                                  ]\n",
    "        itra_num_rows = itra_df.shape[0]\n",
    "        print(\"ITRA rows:\", itra_num_rows)\n",
    "    \n",
    "        ##################################\n",
    "        duv_df = TORX_duv_df[(TORX_duv_df['Race'] == race) &\n",
    "                           (TORX_duv_df['Year'] == year) ]\n",
    "        duv_num_rows = duv_df.shape[0]\n",
    "        print(\"DUV rows:\", duv_num_rows)\n",
    "        \n",
    "        ##################################\n",
    "        trail_df = TORX_100x100_df[(TORX_100x100_df['Race'] == race) &\n",
    "                           (TORX_100x100_df['Year'] == year) &\n",
    "                                  (TORX_100x100_df['Status'] == True)]       \n",
    "        trail_num_rows = trail_df.shape[0]\n",
    "        print(\"100x100 rows:\", trail_num_rows)\n",
    "        ##################################\n",
    "\n",
    "        print(itra_num_rows == duv_num_rows)\n",
    "        print(itra_num_rows == trail_num_rows)\n",
    "        print('*'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a5358",
   "metadata": {},
   "source": [
    "#### Finding similar names for all finishers in 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2022'\n",
    "race = 'TOR330'\n",
    "\n",
    "\n",
    "itra_df = TORX_itra_no_DNF[(TORX_itra_no_DNF['Race'] == race) &\n",
    "                           (TORX_itra_no_DNF['Year'] == year) &\n",
    "                           # not looking at people who Finished at Rifugio Frassati or Bossess\n",
    "                           (TORX_itra_no_DNF['Status'] == 'Finished') \n",
    "                          ]\n",
    "\n",
    "duv_df = TORX_duv_df[(TORX_duv_df['Race'] == race) &\n",
    "                   (TORX_duv_df['Year'] == year) ]\n",
    "\n",
    "\n",
    "trail_df = TORX_100x100_df[(TORX_100x100_df['Race'] == race) &\n",
    "                   (TORX_100x100_df['Year'] == year) &\n",
    "                          (TORX_100x100_df['Status'] == True)\n",
    "                         ]   \n",
    "   \n",
    "print(itra_df.shape)\n",
    "print(duv_df.shape)\n",
    "print(trail_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# itra_df.to_csv('itra.csv', index =  False)\n",
    "# duv_df.to_csv('duv.csv', index =  False)\n",
    "# trail_df.to_csv('trail.cv', index =  False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce60e4c",
   "metadata": {},
   "source": [
    "### Cleaning Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d93ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dictionary to track issues with names\n",
    "name_issues = {}\n",
    "name_issues_dataset = {}\n",
    "\n",
    "dataset_names = ['TORX_100x100', 'TORX_duv', 'TORX_itra_no_DNF']  # Corresponding names of the datasets\n",
    "\n",
    "# Create a dictionary to store the dataset origin for each name\n",
    "name_issues_with_dataset = {}\n",
    "\n",
    "for dataset, dataset_name in zip(datasets, dataset_names):\n",
    "    unique_name = dataset['Name'].unique()\n",
    "    print(f\"Processing {dataset_name}, Unique Names: {len(unique_name)}\")\n",
    "\n",
    "    for name in unique_name: \n",
    "        name_split = name.split(' ')\n",
    "        name1 = name_split[0]\n",
    "        name2 = name_split[1]\n",
    "        name_issues_with_dataset[name] = {'variations': [], 'dataset': dataset_name}\n",
    "\n",
    "        try:\n",
    "            if len(name_split) == 2:\n",
    "                new_name = name2 + ' ' + name1\n",
    "                name_issues_with_dataset[name]['variations'].append(name)\n",
    "                name_issues_with_dataset[name]['variations'].append(new_name)\n",
    "\n",
    "            elif len(name_split) == 3:\n",
    "                # Regex pattern extracting 'D Haene Francois'  \n",
    "                pattern = r\"\\b[A-Za-z]\\s[A-Za-z]+\\s[A-Za-z]+\\b\"\n",
    "\n",
    "                name3 = name_split[2]\n",
    "                if name3 == name1:\n",
    "                    new_name = name1 + ' ' + name2\n",
    "                    name_issues_with_dataset[name]['variations'].append(new_name)\n",
    "\n",
    "                # D Angelo Yuri ['DAngelo Yuri','Yuri DAngelo']\n",
    "                elif pd.Series([name]).str.contains(pattern, regex=True).any():\n",
    "                    new_name = name.replace(f\"{name1} \", f\"{name1}\")\n",
    "                    name_issues_with_dataset[name]['variations'].append(new_name)\n",
    "    \n",
    "                    name_split = new_name.split(' ')\n",
    "                    name1 = name_split[0]\n",
    "                    name2 = name_split[1]\n",
    "                    new_name = name2 + ' ' + name1\n",
    "                    name_issues_with_dataset[name]['variations'].append(new_name)\n",
    "\n",
    "                else:\n",
    "                    new_name1 = name1 + ' ' + name2\n",
    "                    new_name2 = name1 + ' ' + name3\n",
    "                    name_issues_with_dataset[name]['variations'].append(new_name1)\n",
    "                    name_issues_with_dataset[name]['variations'].append(new_name2)\n",
    "\n",
    "            elif len(name_split) > 3:\n",
    "                name_issues_with_dataset[name]['variations'].append(name)\n",
    "\n",
    "            else:\n",
    "                name_issues_with_dataset[name]['variations'].append(name)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Issue: {name}\\nError: {e}\\n\")\n",
    "            name_issues_with_dataset[name]['variations'].append(name)\n",
    "\n",
    "# Convert name issues with dataset to DataFrame for easier tracking\n",
    "name_issues_df = pd.DataFrame([{\n",
    "    'Name': k,\n",
    "    'Variations': v['variations'],\n",
    "    'Dataset': v['dataset']\n",
    "} for k, v in name_issues_with_dataset.items()])\n",
    "\n",
    "print(\"Name Issues DataFrame with Dataset Info:\")\n",
    "print(name_issues_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f542ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee44dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each name and variations\n",
    "for idx, name in enumerate(name_issues_df['Name']):\n",
    "    # Access the current list of variations\n",
    "    current_variations = name_issues_df.at[idx, 'Variations']\n",
    "    # reorder in a alphabetical order\n",
    "    current_variations = sorted(current_variations)\n",
    "    print(current_variations)\n",
    "    \n",
    "    name_issues_df.at[idx, 'Variations'] = current_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59373f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a function to normalize names\n",
    "def normalize_name(name,  name_issues_df ):\n",
    "    # Find the variation list corresponding to the name\n",
    "    match =  name_issues_df [ name_issues_df ['Name'] == name]\n",
    "    if not match.empty:\n",
    "        return match['Variations'].values[0]\n",
    "    return [name]  # If no variation is found, return the name as is.\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Step 2: Apply the name normalization to both datasets\n",
    "    # Normalize dataset1\n",
    "    dataset['Name Variations'] = dataset['Name'].apply(lambda x: normalize_name(x, name_issues_df))\n",
    "\n",
    "TORX_100x100_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd9c740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22b2a109",
   "metadata": {},
   "source": [
    "### Testing Testing Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28c860",
   "metadata": {},
   "source": [
    "### Sabrina Verjee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9faf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Iterate through the names in Dataset1 and Dataset2 to find matches\n",
    "merged_rows = []\n",
    "\n",
    "def testing_df(name):\n",
    "    itra_name_df = TORX_itra_no_DNF[TORX_itra_no_DNF['Name'].str.contains(name)]\n",
    "#     duv_name_df = TORX_duv_df[TORX_duv_df['Name'].str.contains(name)]\n",
    "    trail_name_df = TORX_100x100_df[TORX_100x100_df['Name'].str.contains(name)]\n",
    "\n",
    "    for idx1, row_itra in itra_name_df.iterrows():\n",
    "\n",
    "        for idx2, row_trail in trail_name_df .iterrows():\n",
    "            # Check if the name from itra_verjee_df is in the variations of trail_verjee_df and vice versa\n",
    "            if any(name in row_trail['Name Variations'] for name in row_itra['Name Variations']):\n",
    "\n",
    "                if  row_trail['Year'] ==  row_itra['Year']  :\n",
    "#                     print('Same year:', row_trail['Year'], row_itra['Year'], '\\n', '*'*20)\n",
    "    #                 print('row_itra',row_itra)\n",
    "    #                 print('row_trail',row_trail)\n",
    "\n",
    "                    # Merge the row if a match is found\n",
    "                    merged_row = {**row_itra.to_dict(), **row_trail.to_dict()}\n",
    "                    merged_rows.append(merged_row)\n",
    "    \n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "#     print(merged_rows)\n",
    "    # Step 4: Create a merged DataFrame\n",
    "    merged_df = pd.DataFrame(merged_rows)\n",
    "\n",
    "    merged_df = merged_df.drop_duplicates(subset=['Name','Year','Start Date'])\n",
    "\n",
    "\n",
    "    # Display the merged DataFrame\n",
    "    print(merged_df)\n",
    "    \n",
    "testing_df('Verjee')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5175cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df('Tierney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc4b07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "241cf854",
   "metadata": {},
   "source": [
    "### Merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63eba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Iterate through the names in Dataset1 and Dataset2 to find matches\n",
    "merged_rows = []\n",
    "subname_list= []\n",
    "\n",
    "def testing_df(itra_name_df,trail_name_df):\n",
    "    i=0\n",
    "    for idx1, row_itra in itra_name_df.iterrows():\n",
    "        i = i + 1\n",
    "        print(i)\n",
    "\n",
    "        for idx2, row_trail in trail_name_df .iterrows():\n",
    "            # Check if the name from itra_verjee_df is in the variations of trail_verjee_df and vice versa\n",
    "            if any(name in row_trail['Name Variations'] for name in row_itra['Name Variations']):\n",
    "\n",
    "                if  row_trail['Year'] ==  row_itra['Year']  :\n",
    "#                     print('Same year:', row_trail['Year'], row_itra['Year'], '\\n', '*'*20)\n",
    "    #                 print('row_itra',row_itra)\n",
    "    #                 print('row_trail',row_trail)\n",
    "\n",
    "                    # Merge the row if a match is found\n",
    "                    merged_row = {**row_itra.to_dict(), **row_trail.to_dict()}\n",
    "                    merged_rows.append(merged_row)\n",
    "    \n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                print(row_trail['Name'])\n",
    "                subname_list.append(row_trail['Name'])\n",
    "\n",
    "#     print(merged_rows)\n",
    "    # Step 4: Create a merged DataFrame\n",
    "    merged_df = pd.DataFrame(merged_rows)\n",
    "\n",
    "    merged_df = merged_df.drop_duplicates(subset=['Name','Year','Start Date'])\n",
    "\n",
    "\n",
    "    # Display the merged DataFrame\n",
    "    print(merged_df)\n",
    "    return merged_df, subname_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da11049b",
   "metadata": {},
   "source": [
    "### Testing 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29142e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffa79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(years)\n",
    "sub_TORX_itra_no_DNF = TORX_itra_no_DNF[(TORX_itra_no_DNF['Year'].isin(years)) &\n",
    "                                        (TORX_itra_no_DNF['Status']== 'Finished')\n",
    "                                       ]\n",
    "sub_TORX_100x100_df = TORX_100x100_df[(TORX_100x100_df['Year'].isin(years)) &\n",
    "                                      (TORX_100x100_df['Race'] !=  'TOR130') &\n",
    "                                      (TORX_100x100_df['Status'] ==  True) \n",
    "                                     ]\n",
    "\n",
    "print(sub_TORX_itra_no_DNF.shape)\n",
    "print(sub_TORX_100x100_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d994a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df, subname_list = testing_df(sub_TORX_itra_no_DNF, sub_TORX_100x100_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subname_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b954757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### who is missing?\n",
    "\n",
    "# merged_df_names = merged_df['Name'].unique() \n",
    "# sub_names =sub_TORX_100x100_df['Name'].unique()\n",
    "# print(len(merged_df_names))\n",
    "# print(len(sub_names))\n",
    "\n",
    "# subname_list = []\n",
    "# for sub_name in sub_names:\n",
    "#     if sub_name in  merged_df_names:\n",
    "#         pass\n",
    "#     else:\n",
    "#         subname_list.append(sub_name)\n",
    "        \n",
    "# print(len(subname_list))\n",
    "        \n",
    "        \n",
    "# sub_TORX_100x100_df[sub_TORX_100x100_df['Name'].isin(subname_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74bf3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de8ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275bbb05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e31c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04585c9f",
   "metadata": {},
   "source": [
    "###  Datasets post-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e62c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_2020_TORX_100x100_df = TORX_100x100_df[TORX_100x100_df['Year'].isin(years)]\n",
    "post_2020_TORX_duv_df = TORX_duv_df[TORX_duv_df['Year'].isin(years)]\n",
    "post_2020_TORX_itra_no_DNF = TORX_itra_no_DNF[TORX_itra_no_DNF['Year'].isin(years)]\n",
    "\n",
    "\n",
    "print(post_2020_TORX_100x100_df.head())\n",
    "print(post_2020_TORX_duv_df.head())\n",
    "print(post_2020_TORX_itra_no_DNF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ba2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean names with three parts\n",
    "def cleaning_name_with_3(name):\n",
    "    # Split the name into parts\n",
    "    name_split = name.split(' ')\n",
    "    name1 = name_split[0]\n",
    "    name2 = name_split[1]\n",
    "\n",
    "    # If the name has exactly 3 parts\n",
    "    if len(name_split) == 3:\n",
    "        name3 = name_split[2]\n",
    "\n",
    "        # Regex pattern to identify names with an initial\n",
    "        pattern = r\"^[A-Za-z]\\s[A-Za-z]+\\s[A-Za-z]+$\"\n",
    "\n",
    "        # Check for specific cases\n",
    "        if name3 == name1:\n",
    "            return f\"{name1} {name2}\"\n",
    "\n",
    "        elif re.match(pattern, name):\n",
    "            # Adjust formatting for names like 'D Angelo Yuri'\n",
    "            return name.replace(f\"{name1} \", f\"{name1}\")\n",
    "\n",
    "        else:\n",
    "            # Default to dropping the middle name\n",
    "            return f\"{name1} {name2}\"\n",
    "\n",
    "    # If the name does not have exactly 3 parts, return it unchanged\n",
    "    return name\n",
    "\n",
    "# Apply the function to each dataset\n",
    "for dataset in datasets:\n",
    "    dataset['Name'] = dataset['Name'].apply(cleaning_name_with_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_itra_no_DNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging on two columns: 'Race' and 'Year'\n",
    "merged_df = pd.merge(TORX_duv_df, TORX_itra_no_DNF, on=['Name', 'Race', 'Year'], how='outer')\n",
    "print(TORX_duv_df.shape)\n",
    "print(TORX_itra_no_DNF.shape)\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9303fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ed4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(merged_df['Name'][\n",
    "    (merged_df['ITRA_Performance_Seconds'].isna()) |\n",
    "    (merged_df['DUV_Performance_Seconds'].isna())\n",
    "].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORX_itra_no_DNF[TORX_itra_no_DNF['Name'] == 'Corsini Simone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a7544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b73c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbc17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5857dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56c664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
