{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b17f9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af20bc",
   "metadata": {},
   "source": [
    "### Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527520a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "race = 'TOR330'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f27f311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOR330_dem = pd.read_excel(f'TOR330 Data/5. Clean Data for Data Visualisation/TOR330_dem.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26fd723f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Retired'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Retired'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m TOR330_dem[(TOR330_dem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m----> 2\u001b[0m                (\u001b[43mlifebase_bib_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRetired\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBosses\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3632\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3633\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3634\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Retired'"
     ]
    }
   ],
   "source": [
    "TOR330_dem[(TOR330_dem['Year'] == '2022') &\n",
    "               (TOR330_dem['Retired'] == 'Bosses')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17dc2fdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TOR330_dem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTOR330_dem\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby( [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus1\u001b[39m\u001b[38;5;124m'\u001b[39m] )[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus1\u001b[39m\u001b[38;5;124m'\u001b[39m ]\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TOR330_dem' is not defined"
     ]
    }
   ],
   "source": [
    "TOR330_dem.groupby( ['Year','Status1'] )['Status1' ].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifebase_cut_offs_df = pd.read_excel(f'{race} Data/4. TOR330 Timetable Data/{race}_lifebase_cut_offs_df.xlsx')\n",
    "\n",
    "# # # Convert integer seconds to timedelta\n",
    "lifebase_cut_offs_df['Lifebase Duration'] = pd.to_timedelta(lifebase_cut_offs_df['Lifebase Duration_seconds'], unit='s')\n",
    "\n",
    "# # # Convert integer seconds to timedelta\n",
    "lifebase_cut_offs_df['Running Total Lifebase Duration'] = pd.to_timedelta(lifebase_cut_offs_df['Running Total Lifebase Duration_seconds'], unit='s')\n",
    "\n",
    "lifebase_cut_offs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf473db",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_bib_df = pd.read_excel(f'{race} Data/5. Clean Data for Data Visualisation/{race}_checkpoints_bib_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aid_station_bib_df = pd.read_excel(f'TOR330 Data/5. Clean Data for Data Visualisation/TOR330_all_aid_station_bib_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e327c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifebase_bib_df = pd.read_excel(f'{race} Data/5. Clean Data for Data Visualisation/{race}_lifebase_bib_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399ff01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf897377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # making duration hours \n",
    "# datasets = [checkpoints_bib_df, lifebase_bib_df, all_aid_station_bib_df,  TOR330_dem]\n",
    "# for df in datasets:\n",
    "#     df['Duration_hours'] = df['Duration_seconds']/ 3600 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966f38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aid_station_bib_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c20c80",
   "metadata": {},
   "source": [
    "### Getting Average and Median time for lifebases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d514ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stage1 = [ 'Start', 'Baite Youlaz', 'La Thuile', 'Rifugio Deffeyes',\\\n",
    "          'Planaval', 'Valgrisenche IN']\n",
    "Stage2 = [ 'Valgrisenche OUT', 'Chalet Epee',\\\n",
    "          'Rhemes-Notre-Dame', 'Eaux Rousse', 'Rifugio Sella', 'Cogne IN']\n",
    "Stage3 = [  'Cogne OUT', 'Goilles', 'Rifugio Dondena', 'Chardonney', 'Pontboset','Donnas IN']\n",
    "Stage4 = [  'Donnas OUT', 'Perloz', 'Sassa', 'Rifugio Coda', \\\n",
    "          'Rifugio della Barma', 'Lago Chiaro', 'Col della Vecchia',\\\n",
    "          'Niel La Gruba', 'Loo', 'Gressoney IN']\n",
    "Stage5 = [  'Gressoney OUT', 'Rifugio Alpenzu', 'Champoluc' ,\\\n",
    "          'Rifugio Grand Tournalin', 'Valtournenche IN']\n",
    "Stage6 = [   'Valtournenche OUT', 'Rifugio Barmasse', 'Vareton',\\\n",
    "          'Rifugio Magià', 'Rifugio Cuney', 'Bivacco R. Clermont', 'Oyace', \\\n",
    "          'Bruson Arp', 'Ollomont IN']    \n",
    "Stage7 = [ 'Ollomont OUT', 'Rifugio Champillon', 'Ponteille Desot',\\\n",
    "          'Bosses', 'Rifugio Frassati', 'Pas Entre Deux Sauts',\\\n",
    "          'Monte de la Saxe', 'FINISH']\n",
    "\n",
    "\n",
    "stages =[ Stage1, Stage2, Stage3, Stage4, Stage5, Stage6, Stage7]\n",
    "stages_str =[ 'Stage 1', 'Stage 2', 'Stage 3', 'Stage 4', 'Stage 5', 'Stage 6', 'Stage 7']\n",
    "lifebase_time_spent = ['Valgrisenche OUT','Cogne OUT','Donnas OUT','Gressoney OUT','Valtournenche OUT','Ollomont OUT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_time_stats(df, column, category_order):\n",
    "    \n",
    "    df_merge = df.merge(\n",
    "        TOR330_dem[['PK', 'Finish Category']],\n",
    "        on=['PK'],\n",
    "        how='left')\n",
    "\n",
    "\n",
    "    df_merge['Duration_seconds'][df_merge['Duration_seconds']  <= 0] = np.nan\n",
    "\n",
    "    stats_df = df_merge.groupby(['Finish Category', column])['Duration_seconds'].describe().reset_index(drop = False)\n",
    "\n",
    "\n",
    "    stats_df[[ 'mean', 'std', 'min', '25%',\n",
    "           '50%', '75%', 'max']] = stats_df[[ 'mean', 'std', 'min', '25%',\n",
    "           '50%', '75%', 'max']].round(0)\n",
    "\n",
    "    # Set 'Finish Category' as a categorical column with the defined order\n",
    "    stats_df[column] = pd.Categorical(\n",
    "        stats_df[column],\n",
    "        categories=category_order,\n",
    "        ordered=True)\n",
    "    \n",
    "    stats_df = stats_df.sort_values(by=column, ascending = True)\n",
    "    \n",
    "    \n",
    "\n",
    "    for finish_category in df_merge['Finish Category'].unique():\n",
    "        print(finish_category)\n",
    "        stats_df.loc[\n",
    "                (stats_df['Finish Category'] == finish_category), 'running_total_mean_seconds'\n",
    "            ] =     stats_df.loc[\n",
    "                (stats_df['Finish Category'] == finish_category), 'mean'\n",
    "            ].cumsum()\n",
    "\n",
    "\n",
    "        stats_df.loc[\n",
    "                (stats_df['Finish Category'] == finish_category), 'running_total_median_seconds'\n",
    "            ] =     stats_df.loc[\n",
    "                (stats_df['Finish Category'] == finish_category), '50%'\n",
    "            ].cumsum()\n",
    "\n",
    "\n",
    "    stats_df = stats_df[['Finish Category', column,\n",
    "            'count', 'mean', '50%', 'std', 'min', 'max', \n",
    "            'running_total_mean_seconds', 'running_total_median_seconds']]\n",
    "\n",
    "\n",
    "    stats_df = stats_df.rename(columns={'count': f'Count_Finish_Category_{column}_seconds',\n",
    "                                      'mean': f'Mean_Finish_Category_{column}_seconds',\n",
    "                                      'std': f'STD_Finish_Category{column}t_seconds',\n",
    "                                      '50%': f'Median_Finish_Category_{column}_seconds',\n",
    "                                      'min': f'Min_Finish_Category_{column}_seconds', \n",
    "                                      'max': f'Max_Finish_Category_{column}_seconds'})\n",
    "    stats_df = stats_df[stats_df[column] != 'Start']\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    for stage,  stage_str in zip(stages, stages_str):\n",
    "        stats_df.loc[stats_df[column].isin(stage), 'Stage'] = f'{stage_str}'\n",
    "        \n",
    "    for lifebase in lifebase_time_spent:\n",
    "        lifebase_split = lifebase.split(' OUT')[0] \n",
    "        stats_df.loc[stats_df[column] == lifebase, 'Stage'] = f'Time Spent in {lifebase_split}'\n",
    "\n",
    "\n",
    "    stats_df.to_excel(f'{race} Data/5. Clean Data for Data Visualisation/{race}_{column}_duration_mean_median_stats_df.xlsx', index = False)\n",
    "\n",
    "#     print(stats_df[stats_df['Finish Category'] == 'Sub-130'])\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifebase_stats_df = creating_time_stats(df = lifebase_bib_df, \n",
    "                    column = 'Lifebase', \n",
    "                    category_order = ['Start','Valgrisenche IN','Valgrisenche OUT',\n",
    "                            'Cogne IN',  'Cogne OUT',\n",
    "                            'Donnas IN', 'Donnas OUT', \n",
    "                            'Gressoney IN','Gressoney OUT',\n",
    "                            'Valtournenche IN','Valtournenche OUT', \n",
    "                            'Ollomont IN','Ollomont OUT',\n",
    "                            'FINISH'])\n",
    "\n",
    "lifebase_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Convert integer seconds to timedelta\n",
    "lifebase_stats_df['Median_Finish_Category_Lifebase'] = pd.to_timedelta(lifebase_stats_df['Median_Finish_Category_Lifebase_seconds'], unit='s')\n",
    "\n",
    "\n",
    "lifebase_stats_df[['Finish Category','Lifebase', 'Median_Finish_Category_Lifebase']][lifebase_stats_df['Finish Category'] == 'Sub-130']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac72d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_stats_df = creating_time_stats(df = checkpoints_bib_df, \n",
    "                    column = 'Checkpoint', \n",
    "                    category_order = ['La Thuile', 'Valgrisenche IN', 'Valgrisenche OUT',\n",
    "                                       'Eaux Rousse', 'Cogne IN', 'Cogne OUT', 'Donnas IN', 'Donnas OUT',\n",
    "                                       'Rifugio della Barma', 'Niel La Gruba', 'Gressoney IN',\n",
    "                                       'Gressoney OUT', 'Champoluc', 'Valtournenche IN',\n",
    "                                       'Valtournenche OUT', 'Oyace', 'Ollomont IN', 'Ollomont OUT',\n",
    "                                       'FINISH'])\n",
    "\n",
    "checkpoints_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_aid_station_bib_stats_df = creating_time_stats(df = all_aid_station_bib_df, \n",
    "#                     column = 'Aid Station', \n",
    "#                     category_order = ['Baite Youlaz', 'La Thuile', 'Rifugio Deffeyes',\n",
    "#                                    'Planaval', 'Valgrisenche IN', 'Valgrisenche OUT', 'Chalet Epee',\n",
    "#                                    'Rhemes-Notre-Dame', 'Eaux Rousse', 'Rifugio Sella', 'Cogne IN',\n",
    "#                                    'Cogne OUT', 'Goilles', 'Rifugio Dondena', 'Chardonney', 'Pontboset',\n",
    "#                                    'Donnas IN', 'Donnas OUT', 'Perloz', 'Sassa', 'Rifugio Coda',\n",
    "#                                    'Rifugio della Barma', 'Lago Chiaro', 'Col della Vecchia',\n",
    "#                                    'Niel La Gruba', 'Loo', 'Gressoney IN', 'Gressoney OUT',\n",
    "#                                    'Rifugio Alpenzu', 'Champoluc', 'Rifugio Grand Tournalin',\n",
    "#                                    'Valtournenche IN', 'Valtournenche OUT', 'Rifugio Barmasse', 'Vareton',\n",
    "#                                    'Rifugio Magià', 'Rifugio Cuney', 'Bivacco R. Clermont', 'Oyace',\n",
    "#                                    'Bruson Arp', 'Ollomont IN', 'Ollomont OUT', 'Rifugio Champillon',\n",
    "#                                    'Ponteille Desot', 'Bosses', 'Rifugio Frassati', 'Pas Entre Deux Sauts',\n",
    "#                                    'Monte de la Saxe', 'FINISH'])\n",
    "\n",
    "# all_aid_station_bib_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72508766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa1c902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13053ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f36b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cde07a13",
   "metadata": {},
   "source": [
    "### Who ran too easy or hard at the start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_TOR330_dem_bib_list = list(TOR330_dem['PK'][TOR330_dem['Finish Category'] == 'Sub-130'].unique())\n",
    "\n",
    "# new_bib_list = []\n",
    "# for bib in sub_TOR330_dem_bib_list:\n",
    "#     df = lifebase_bib_df[\n",
    "#                 ((lifebase_bib_df['Duration_hours']> 14) &\n",
    "#                 (lifebase_bib_df['Lifebase'] == 'Valgrisenche IN') )\n",
    "#     &\n",
    "#                 (lifebase_bib_df['PK']== bib)\n",
    "#                ]\n",
    "    \n",
    "#     new_bib_list.append(df)\n",
    "# df= pd.concat(new_bib_list)\n",
    "# pk_unique = list(df['PK'].unique())\n",
    "\n",
    "# for pk in pk_unique:\n",
    "#     print(lifebase_bib_df[['PK','Lifebase', 'Timestamp', 'Duration_hours']][(lifebase_bib_df['PK'] == pk)  ], '\\n', '*'*40,  '\\n',)\n",
    "#     print(TOR330_dem[['PK', 'Finish Category', 'Duration_hours']][(TOR330_dem['PK'] == pk)  ], '\\n','\\n','\\n','\\n', '*'*40,  '\\n',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaee483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_TOR330_dem_bib_list = list(TOR330_dem['PK'][TOR330_dem['Finish Category'] == 'Sub-140'].unique())\n",
    "\n",
    "# new_bib_list = []\n",
    "# for bib in sub_TOR330_dem_bib_list:\n",
    "#     df = lifebase_bib_df[\n",
    "#                 ((lifebase_bib_df['Duration_hours']< 10) &\n",
    "#                 (lifebase_bib_df['Lifebase'] == 'Valgrisenche IN') ) &\n",
    "#                 (lifebase_bib_df['PK']== bib)]\n",
    "#     new_bib_list.append(df)\n",
    "    \n",
    "# df= pd.concat(new_bib_list)\n",
    "\n",
    "# pk_unique = list(df['PK'].unique())\n",
    "\n",
    "# for pk in pk_unique:\n",
    "#     print(lifebase_bib_df[['PK', 'Wave', 'Lifebase', 'Duration_hours']][(lifebase_bib_df['PK'] == pk)  ], '\\n', '\\n',)\n",
    "#     print(TOR330_dem[['PK', 'Finish Category', 'Duration_hours']][(TOR330_dem['PK'] == pk)  ], '\\n', '*'*40,  '\\n',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reading in Raw Data\n",
    "# races = ['TOR330']\n",
    "# years = [ \n",
    "# #     '2021',\n",
    "# #         '2022',\n",
    "# #          '2023', \n",
    "#     '2024'\n",
    "#         ]\n",
    "\n",
    "# TORX_df = {}\n",
    "\n",
    "# for race in races:\n",
    "#     for year in years:\n",
    "#         df = pd.read_excel(f'{race} Data/1. 100x100trail/{race}_{year}.xlsx',\n",
    "#                                  dtype={'Start Date': 'string',\n",
    "#                                         'Year': 'string'})\n",
    "#         print(f'{race}_{year} {df.shape}')\n",
    "#         # Store the DataFrame in the dictionary with a key like 'TOR330_2021'\n",
    "#         TORX_df[f'{race}_{year}'] = df\n",
    "#     print('*'*50)\n",
    "    \n",
    "# TORX_df_concat = pd.concat(TORX_df)\n",
    "# TOR330 = TORX_df_concat[TORX_df_concat['Year'] == year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a170581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_TOR330_dem_bib_list = list(TOR330_dem['PK'][TOR330_dem['Finish Category'] == 'Sub-90'].unique())\n",
    "\n",
    "# new_bib_list = []\n",
    "# for bib in sub_TOR330_dem_bib_list:\n",
    "#     df = lifebase_bib_df[\n",
    "#                 ((lifebase_bib_df['Duration_hours']> 8) &\n",
    "#                 (lifebase_bib_df['Lifebase'] == 'Valgrisenche OUT') ) &\n",
    "#                 (lifebase_bib_df['PK']== bib)]\n",
    "#     new_bib_list.append(df)\n",
    "    \n",
    "# df= pd.concat(new_bib_list)\n",
    "\n",
    "# pk_unique = list(df['PK'].unique())\n",
    "\n",
    "# for pk in pk_unique:\n",
    "#     print(lifebase_bib_df[['PK', 'Wave', 'Lifebase', 'Duration_hours']][(lifebase_bib_df['PK'] == pk)  ], '\\n', '\\n',)\n",
    "#     print(TOR330_dem[['PK', 'Finish Category', 'Duration_hours']][(TOR330_dem['PK'] == pk)  ], '\\n', '*'*40,  '\\n',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9dd748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf6533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
